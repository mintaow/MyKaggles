{"cells":[{"metadata":{"id":"IIybku08dCvL","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as  np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport json\nimport datetime\nfrom sklearn import preprocessing\nsns.set()\n\nimport joblib\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Summary\n## Feature Engineer:\n1. 构造了所有categorical variable的**count 特征**（count for every unique value)\n2. 用前一天的数据构造了**点击率**与**曝光数**特征\n3. 基于部分特征构造了**二阶统计特征**\n\n## Memory Reduction:\n1. 将数据转化成不同的格式\n2. 注意删除不再使用的变量\n\nResults: Logloss 0.3907; Ranking 158th; Public board Top10% (late submission)"},{"metadata":{"id":"AdQUHB9_eON3"},"cell_type":"markdown","source":"# 1. Load the data"},{"metadata":{"id":"9ZRht41jdfrB","outputId":"eb316712-9d8a-479e-c280-9dde3b7bd9de","trusted":true},"cell_type":"code","source":"num_of_chunk = 0\nchunksize = 10 ** 6\n\ntrain = pd.DataFrame()\ntrain_path = \"../input/avazu-ctr-prediction/train.gz\"\ntest_path = \"../input/avazu-ctr-prediction/test.gz\"\n\nfor chunk in pd.read_csv(train_path, sep = \",\", chunksize=chunksize):\n    num_of_chunk += 1\n    train = pd.concat([train, chunk.sample(frac=.01, replace=False, random_state=123)], axis=0) # >= chunk.sample(frac = .002)\n    print('Processing Chunk No. ' + str(num_of_chunk))     \n    \ntrain.reset_index(inplace=True)\n\n# Create a back-up file for the length of the training set\ntrain_len = len(train)\ntrain_len","execution_count":null,"outputs":[]},{"metadata":{"id":"FMrbDtBVeH40","trusted":false},"cell_type":"code","source":"test = pd.read_csv(test_path, sep = \",\", delimiter=',',dtype={'id': str})\ndata = pd.concat([train,test],sort= False)","execution_count":null,"outputs":[]},{"metadata":{"id":"wVagUUmReH2i","trusted":false},"cell_type":"code","source":"def reduce_mem(df):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024 ** 2\n    print('{:.2f} Mb, {:.2f} Mb ({:.2f} %)'.format(start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    gc.collect()\n    return df","execution_count":null,"outputs":[]},{"metadata":{"id":"hAT1MSAoiDQB","trusted":false},"cell_type":"code","source":"data.memory_usage()","execution_count":null,"outputs":[]},{"metadata":{"id":"5VvpxGGleH0K","trusted":false},"cell_type":"code","source":"data = reduce_mem(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"GeElpvN6eHxy","outputId":"eba21910-14ee-4697-d103-f18007fb56b7","trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"VZwerNJIrPCZ"},"cell_type":"markdown","source":"# 2. Data Processing\nUse only date > 27 for model training"},{"metadata":{"id":"GI4AyTpRgMDR","trusted":false},"cell_type":"code","source":"# get the data\ndef get_date(x):\n    return str(x)[4:6]\ndata['day'] = data['hour'].apply(get_date)\ndata['day'] = data['day'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"id":"yn4CL_UZrO0G","trusted":false},"cell_type":"code","source":"data = data[data['day']>=27]","execution_count":null,"outputs":[]},{"metadata":{"id":"U_PF2iEmeHvi","trusted":false},"cell_type":"code","source":"del data['id'],data['index']","execution_count":null,"outputs":[]},{"metadata":{"id":"0sXS12MueHqo"},"cell_type":"markdown","source":"# 3. Feature Engineering"},{"metadata":{"id":"spvXooEQrgdZ"},"cell_type":"markdown","source":"### 3-1. Count features for every categorical variable"},{"metadata":{"id":"DPyDJJxcri7b","trusted":false},"cell_type":"code","source":"# Generate user identifier\ndata['user'] = data['device_id'].astype(str) + '-'  + data['device_model'].astype(str)+'-'+ data['device_ip'].astype(str)","execution_count":null,"outputs":[]},{"metadata":{"id":"Y-gWhmYArpe7","outputId":"387651c9-82ce-4a18-d7f2-5a2c2db8995c","trusted":false},"cell_type":"code","source":"cate_fea = ['user','device_id','C1', 'banner_pos', 'site_id', 'site_domain', 'site_category', 'app_id', 'app_domain', 'app_category', 'device_id', 'device_ip', 'device_model', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C18', 'C19', 'C20', 'C21']\nfor f in cate_fea:\n    # generate a dictionary for each unique value, associated with its Id\n    map_dict = dict(zip(data[f].unique(), range(data[f].nunique())))\n    # transform each categorical variable using label encoding (1,2,3,4...)\n    data[f] = data[f].map(map_dict).fillna(-1).astype('int32')\n    # generate the count feature\n    data[f + '_count'] = data[f].map(data[f].value_counts())\n    data = reduce_mem(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"ctykkrWWtRak"},"cell_type":"markdown","source":"Delete unused columns"},{"metadata":{"id":"O8QiRMBTsH4I","trusted":false},"cell_type":"code","source":"for f in ['hour','device_ip','C17']:\n  del data[f]","execution_count":null,"outputs":[]},{"metadata":{"id":"qfzyZ6QstXlK","trusted":false},"cell_type":"code","source":"data = data.reset_index(drop = True)\ndata['id'] = data.index + 1\nclick_df = data[data['click'] == 1]","execution_count":null,"outputs":[]},{"metadata":{"id":"q-xpr8Mdt5Pf","outputId":"66512180-c42f-4207-9a1b-20301188077a","trusted":false},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"id":"MWxZXYcwuQ5L"},"cell_type":"markdown","source":"### 3-2. Click frequency & Impression\nCalculate click & impression at different level of aggregation"},{"metadata":{"id":"Y_-X_7psuQFl","outputId":"2747e251-0111-4366-9c71-ad355df6ffc5","trusted":false},"cell_type":"code","source":"for f in [\n    ['user'],\n    ['banner_pos', 'user'],\n    ['C21', 'user'],\n    # ...\n]:\n    print('------------------ {} ------------------'.format('_'.join(f)))\n    \n    # 对前一天的点击次数进行统计\n    tmp = click_df[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg({'_'.join(f) + '_prev_day_click_count': 'count'})\n    tmp['day'] += 1 # make the above calculated count correspond to previous day\n    data = data.merge(tmp, on=f + ['day'], how='left')\n    data['_'.join(f) + '_prev_day_click_count'] = data['_'.join(f) + '_prev_day_click_count'].fillna(0) # deal with NA in the first day\n    data.loc[data['day'] == 27, '_'.join(f) + '_prev_day_click_count'] = None\n    \n    # 对前一天的曝光量进行统计\n    tmp = data[f + ['day', 'id']].groupby(f + ['day'], as_index=False)['id'].agg({'_'.join(f) + '_prev_day_count': 'count'})\n    tmp['day'] += 1\n    data = data.merge(tmp, on=f + ['day'], how='left')\n    data['_'.join(f) + '_prev_day_count'] = data['_'.join(f) + '_prev_day_count'].fillna(0)\n    data.loc[data['day'] == 27, '_'.join(f) + '_prev_day_count'] = None\n    \n    # 计算前一天的点击率\n    data['_'.join(f) + '_prev_day_ctr'] = data['_'.join(f) + '_prev_day_click_count'] / (\n            data['_'.join(f) + '_prev_day_count'] + data['_'.join(f) + '_prev_day_count'].mean())\n\n    del tmp\n\ndel click_df\n\ndata = reduce_mem(data)","execution_count":null,"outputs":[]},{"metadata":{"id":"0q06I3lK4O8W","outputId":"c9b9ec7d-82cd-4b67-81e8-5fcb7686042a","trusted":false},"cell_type":"code","source":"cols = [\"user_prev_day_click_count\", \n        \"user_prev_day_count\",\n        \"user_prev_day_ctr\", \n        \"banner_pos_user_prev_day_click_count\", \n        \"banner_pos_user_prev_day_count\", \n        \"banner_pos_user_prev_day_ctr\", \"C21_user_prev_day_click_count\", \"C21_user_prev_day_count\", \"C21_user_prev_day_ctr\"]\ndata[cols].describe()","execution_count":null,"outputs":[]},{"metadata":{"id":"IPkJO7uJwr6Z","outputId":"13863ed6-36a7-406b-a977-5929b5a039d2","trusted":false},"cell_type":"code","source":"data  = data.reset_index(drop = True)\ndata['index'] = data.index +1\n\nfrom scipy.stats import entropy\nprint('*************************** cross feat (second order) ***************************')\n# 二阶交叉特征，可以继续做更高阶的交叉。\n# Why not use device type\ncross_cols = ['device_model','app_id','site_id','site_domain','banner_pos']\nfor f in cross_cols:\n    for col in cross_cols:\n        if col == f:\n            # The continue statement directly continues to loop without running the remaining code\n            continue\n        print('------------------ {} {} ------------------'.format(f, col))\n        data = data.merge(data[[f, col]].groupby(f, as_index=False)[col].agg({\n            'cross_{}_{}_nunique'.format(f, col): 'nunique',\n            'cross_{}_{}_ent'.format(f, col): lambda x: entropy(x.value_counts() / x.shape[0]) # 熵\n        }), on=f, how='left')\n        if 'cross_{}_{}_count'.format(f, col) not in data.columns.values and 'cross_{}_{}_count'.format(col, f) not in data.columns.values:\n            data = data.merge(data[[f, col, 'index']].groupby([f, col], as_index=False)['index'].agg({\n                'cross_{}_{}_count'.format(f, col): 'count' # 共现次数\n            }), on=[f, col], how='left')\n        if 'cross_{}_{}_count_ratio'.format(col, f) not in data.columns.values:\n            data['cross_{}_{}_count_ratio'.format(col, f)] = data['cross_{}_{}_count'.format(f, col)] / data[f + '_count'] # 比例偏好\n        if 'cross_{}_{}_count_ratio'.format(f, col) not in data.columns.values:\n            data['cross_{}_{}_count_ratio'.format(f, col)] = data['cross_{}_{}_count'.format(f, col)] / data[col + '_count'] # 比例偏好\n        data['cross_{}_{}_nunique_ratio_{}_count'.format(f, col, f)] = data['cross_{}_{}_nunique'.format(f, col)] / data[f + '_count']\n    data = reduce_mem(data)\ndel data['index']\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"thpkgw1E5Uld"},"cell_type":"markdown","source":"# 4. Modeling"},{"metadata":{"id":"7UmTRFKL7rzs","trusted":false},"cell_type":"code","source":"train_df = data[data['click'].isna()==False].reset_index(drop=True)\ntest_df = data[data['click'].isna()==True].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"LNSoV5lB7wgu","outputId":"b8f076d8-0dfb-4982-cd18-24d310510a96","trusted":false},"cell_type":"code","source":"X_train = train_df[train_df[\"day\"]<30].copy()\ny_train = X_train[\"click\"].astype('int8')\nX_valid = train_df[train_df[\"day\"] ==30]\ny_valid = X_valid[\"click\"].astype('int8')\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"MGd_Q7sv7--h","outputId":"50a38913-e920-4947-b6f1-c0e43c3d601d","trusted":false},"cell_type":"code","source":"drop_fea = ['day','click']\nfeature= [x for x in train_df.columns if x not in drop_fea]\nprint(len(feature))\nprint(feature)\n\ndel data\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"id":"KB4CFR0C8NRF"},"cell_type":"markdown","source":"### 4-1. Build the CatBoostClassifier\nCatBoost is a high-performance open source library for gradient boosting on decision trees."},{"metadata":{"id":"V3z3ngOfuLQP","outputId":"df1a7ef8-7494-496e-da9d-87a5bc5b0b72","trusted":false},"cell_type":"code","source":"# Install CatBoost\n!pip install catboost\n!pip install ipywidgets\n!jupyter nbextension enable --py widgetsnbextension","execution_count":null,"outputs":[]},{"metadata":{"id":"FSQE2k7n7oKa","trusted":false},"cell_type":"code","source":"from catboost import CatBoostClassifier","execution_count":null,"outputs":[]},{"metadata":{"id":"-_lAA8AA8HdA","outputId":"5c8016c1-123f-46c5-d687-86be51d1692a","trusted":false},"cell_type":"code","source":"clf = CatBoostClassifier(iterations=100000, depth=6,learning_rate=0.01, loss_function='Logloss',cat_features=[]\n                        ,verbose=True,eval_metric='Logloss',counter_calc_method='Full',task_type='GPU',metric_period=50)\nclf.fit(\n    X_train[feature], y_train.astype('int32'),\n    eval_set=[(X_valid[feature],y_valid.astype('int32'))],\n    early_stopping_rounds=200,\n    verbose=True,\n    use_best_model=True,\n)\n","execution_count":null,"outputs":[]},{"metadata":{"id":"pITj62Yw8YqK","trusted":false},"cell_type":"code","source":"import matplotlib.pyplot as plt \nfrom matplotlib import cm\nscore = pd.DataFrame()\nscore['fea_name'] = clf.feature_names_\nscore['fea']=clf.feature_importances_\nscore = score.sort_values(['fea'], ascending=False)\ntemp = pd.DataFrame()\ntemp = score[:60]\ncolor = cm.jet(temp['fea']/temp['fea'].max())\nplt.figure(figsize=(10, 15))\nplt.barh(temp['fea_name'],temp['fea'],height =0.8,color=color,alpha=0.8)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"QcrEnAAe8bZe","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}